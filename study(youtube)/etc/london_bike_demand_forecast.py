# -*- coding: utf-8 -*-
"""london-bike-demand-forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JMT756kEP1cKbztgQXvpd9CFRI_u78nS
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno

df = pd.read_csv('/kaggle/input/london-bike-sharing-dataset/london_merged.csv',
                parse_dates = ["timestamp"])
df.head()

# structure and type of data

print("structure:", df.shape)
print("type:", df.dtypes)
print("columns:", df.columns)

df.isna().sum()

# 결측치가 없다.

msno.matrix(df)
plt.show()

# 결측치를 시각적으로 확인하는 방법

# 열을 추가한다. dt는 pandas에서 제공하는 datetime 객체에 대한 속성 접근자이다.

df['year'] = df['timestamp'].dt.year
df['month'] = df['timestamp'].dt.month
df['dayofweek'] = df['timestamp'].dt.dayofweek
df['hour'] = df['timestamp'].dt.hour
df.head()

# 변수에 대한 값 요약(범주형일 때 유용)

print(df['year'].value_counts(), end = "\n\n")
print(df['month'].value_counts())

a, b = plt.subplots(1, 1, figsize = (10, 5))
sns.boxplot(data = df, x = 'year', y = 'cnt')

a, b = plt.subplots(1, 1, figsize = (10, 5))
sns.boxplot(data = df, x = 'month', y = 'cnt')

a, b = plt.subplots(1, 1, figsize = (10, 5))
sns.boxplot(data = df, x = 'dayofweek', y = 'cnt')

plt.subplots(1, 1, figsize = (15, 10))
sns.boxplot(data = df, x = 'hour', y = 'cnt')

# 그래프 함수 만들기
def plot_bar(data, feature):
    fig = plt.figure(figsize = (10, 10))
    sns.barplot(x = feature, y = "cnt", data = data, palette = "Set3", orient = "v")

plot_bar(df, "hour")

plot_bar(df, "dayofweek")

# remove outliers

def is_outliers(s):
    sigma_3 = s.std() * 3
    lower = s.mean() - sigma_3
    upper = s.mean() + sigma_3
    return ~s.between(lower, upper)

df_out = df[~df.groupby('hour')['cnt'].apply(is_outliers).reset_index(drop = True)]

print("before removing", df.shape)
print("after removing", df_out.shape)

df_out.dtypes

df_out['weather_code'] = df_out["weather_code"].astype('category')
df_out['season'] = df_out["season"].astype('category')
df_out['year'] = df_out["year"].astype('category')
df_out['month'] = df_out["month"].astype('category')
df_out['hour'] = df_out["hour"].astype('category')

df_out.dtypes

df_out.head()

df_out = pd.get_dummies(df_out, columns = ['weather_code', 'season', 'year', 'month', 'hour'])

df_out.head()

df_out.shape

df_y = df_out['cnt']
df_x = df_out.drop(["timestamp", 'cnt'], axis = 1) # axis = 1이면 열(변수)을 제거, 0이면 행(데이터)을 제거
df_x.head()

df_y.head()

# 훈련용, 테스트용 데이터 분리

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, random_state = 66, test_size = 0.3,
                                                   shuffle = False)

print("x_train의 구조는:", x_train.shape)
print("y_train의 구조는:", y_train.shape)

print("x_test 구조는:", x_test.shape)
print("y_test 구조는:", y_test.shape)

# deepLearning

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Dense(units = 160, activation = 'relu', input_dim = 57))
model.add(Dense(units = 60, activation = 'relu'))
model.add(Dense(units = 20, activation = 'relu'))
model.add(Dense(units = 1, activation = 'linear'))

model.summary()

model.compile(loss = 'mae', optimizer = 'adam', metrics = ['mae'])
early_stopping = EarlyStopping(monitor = 'loss', patience = 5, mode = 'min')
history = model.fit(x_train, y_train, epochs = 50, batch_size = 1, validation_split = 0.1, callbacks = [early_stopping])

plt.subplots(1, 1, figsize = (15, 10))

plt.plot(history.history['val_loss'])
plt.plot(history.history['loss'])
plt.xlabel("Ephochs")
plt.legend(["val_loss", "loss"])

# 만든 모델을 이용하여 실제로 예측

y_predict = model.predict(x_test)

from sklearn.metrics import mean_squared_error

def RMSE(y_test, y_predict):
    return np.sqrt(mean_squared_error(y_test, y_predict))

print("RMSE:", RMSE(y_test, y_predict))

# RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = 100, random_state = 16)
rf.fit(x_train, y_train)
rf_result = rf.predict(x_test)
print("RMSE:", RMSE(y_test, rf_result))

# XGBRegressor

from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators = 100, random_state = 16)
xgb.fit(x_train, y_train)
xgb_result = xgb.predict(x_test)
print("RMSE:", RMSE(y_test, xgb_result))

# LightGbm

from lightgbm import LGBMRegressor
lgb = LGBMRegressor(n_estimators = 100, random_state = 16)
lgb.fit(x_train, y_train)
lgb_result = lgb.predict(x_test)
print("RMSE:", RMSE(y_test, lgb_result))

"""## 각 모형들을 비교"""

xgb = pd.DataFrame(xgb_result)
rf = pd.DataFrame(rf_result)
dnn = pd.DataFrame(y_predict)
lgb = pd.DataFrame(lgb_result)
compare = pd.DataFrame(y_test).reset_index(drop = True)

compare.head() # 실제값(cnt)

compare['xgb'] = xgb
compare['rf'] = rf
compare['dnn'] = dnn
compare['lgb'] = lgb
compare.head()

sns.kdeplot(compare['cnt'], shade = True, color = "r")
sns.kdeplot(compare['xgb'], shade = True, color = "b")
sns.kdeplot(compare['rf'], shade = True, color = "y")
sns.kdeplot(compare['dnn'], shade = True, color = "g")
sns.kdeplot(compare['lgb'], shade = True, color = "p")