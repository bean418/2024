# -*- coding: utf-8 -*-
"""basicRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jl6qGTi5WhsMTBLg4goKuPa7o_u5VtH4
"""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/AI_youtube/LinearRegressionData.csv")

df.shape

df.head()

X = df.iloc[:, :-1]
y = df.iloc[:, -1]
print(X, y)

reg = LinearRegression()
reg.fit(X, y)

y_pred = reg.predict(X)
y_pred

plt.scatter(X, y, color = "blue")
plt.plot(X, y_pred, color = "green")
plt.title("Score by hours")
plt.xlabel("hours")
plt.ylabel("score")

print("9시간, 8시간 공부했을 때 예상 점수:", reg.predict([[9], [8]]))

print(reg.coef_, reg.intercept_)

"""# **seperate data set**"""

from sklearn.model_selection import train_test_split

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

help(train_test_split)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
# random_state: set.seed in R

X, len(X)

X_train, len(X_train) # test_size가 0.2이므로 train_size는 0.8, 즉 전체 데이터 개수(20) * 0.8 = 16이다.

X_test, len(X_test)

"""### **Modeling using seperated data**"""

reg = LinearRegression()

reg.fit(X_train, y_train) # 훈련세트로 학습

"""### **Data Visualization**"""

# 훈련세트

plt.scatter(X_train, y_train)
plt.plot(X_train, reg.predict(X_train), color = "green")
plt.title("Score by hours")
plt.xlabel("hours")
plt.ylabel("score")

# 테스트 세트

plt.scatter(X_test, y_test)
plt.plot(X_train, reg.predict(X_train), color = "green")
plt.title("Score by hours")
plt.xlabel("hours")
plt.ylabel("score")

print(reg.coef_, reg.intercept_)

"""### **Model Evaluation**"""

reg.score(X_test, y_test)

plt.scatter(X_test, y_test)
plt.plot(X_test, reg.predict(X_test), color = "red")

plt.plot(X_test, y_test)
plt.plot(X_test, reg.predict(X_test), color = "red")

# reg는 선형회귀모델이므로 데이터가 입력되면 predicted는 모두 한 직선 위에 위치한다.
# 하지만 plt.plot(X_test, y_test) 는 데이터가 한 직선 위에 위치한 것이 아니므로 선을 이으면 아래와 같이 출력된다.

"""## **Gradient Descent**"""

from sklearn.linear_model import SGDRegressor # SGD : Stochastic Gradient Descent
sr = SGDRegressor(max_iter = 100, eta0 = 1e-4, random_state = 0, verbose = 1) # max_iter: ephocs, eta0 = learning rate
sr.fit(X_train, y_train)

plt.scatter(X_train, y_train)
plt.plot(X_train, sr.predict(X_train), color = "green")
plt.title("Score by hours")
plt.xlabel("hours")
plt.ylabel("score")

"""learning rate가 작은데, ephocs가 작으므로 loss가 최저인 직선을 찾지 못 한 상황
loss = 253
"""

sr2 = SGDRegressor(max_iter = 1000, eta0 = 1e-4, random_state = 0, verbose = 1) # max_iter: ephocs, eta0 = learning rate
sr2.fit(X_train, y_train)

plt.scatter(X_train, y_train)
plt.plot(X_train, sr2.predict(X_train), color = "green")
plt.title("Score by hours")
plt.xlabel("hours")
plt.ylabel("score")

"""learning rate가 작지만, ephocs가 1000이기 때문에 loss가 적절히 줄어든 상황"""

print("선형 회귀:", reg.coef_, reg.intercept_)
print("SGD:", sr.coef_, sr.intercept_)

sr.score(X_train, y_train)

sr.score(X_test, y_test)